# ğŸš€ TinyStories LLM â€” Building a Small Language Model from Scratch

<p align="center">
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white"/>
  <img src="https://img.shields.io/badge/Deep%20Learning-%23007ACC?style=for-the-badge&logo=ai&logoColor=white"/>
  <img src="https://img.shields.io/badge/Transformer%20Architecture-%23FFB6C1?style=for-the-badge"/>
</p>

## ğŸ§  Overview  
This project implements a **miniature Large Language Model (LLM)** **from scratch**, trained on the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset.  
The goal is **not** to build another language model â€” but to **understand every moving part**: tokenization, attention, optimization, and text generation.

Itâ€™s a ground-up implementation inspired by GPT-style architectures, written in PyTorch for the purpose of learning.

---

## âœ¨ Key Features
- ğŸ§© **Transformer Architecture** â€” Implements multi-head self-attention, positional encodings, and layer normalization manually.  
- ğŸ‹ï¸ **Training on TinyStories** â€” Trained from scratch on a child-friendly corpus for lightweight yet meaningful language generation.  
- ğŸ§ª **Sampling & Text Generation** â€” Greedy, temperature, and top-k sampling implemented natively.  
- ğŸ“ˆ **Loss Visualization** â€” Real-time tracking of training and validation losses.  
- ğŸ§° **Fully Reproducible** â€” Minimal dependencies, clear code structure, and easily extendable.

---

## ğŸ§¬ Model Architecture
| Component | Description |
|------------|-------------|
| Embedding Layer | Token + positional embeddings |
| Transformer Blocks | Multi-Head Attention + MLP + LayerNorm |
| Context Length | Configurable |
| Parameters | ~4.8M (configurable) |
| Framework | PyTorch |

---

## ğŸ§‘â€ğŸ’» Implementation Highlights
- **No external model libraries** â€” all attention, normalization, and weight initialization coded manually.  
- **Trains on CPU or single GPU** â€” making it accessible to anyone with modest hardware.  

---

## ğŸ“Š Training Performance
| Metric | Value |
|--------|-------|
| Dataset | TinyStories (â‰ˆ 1K sentences) |
| Best Loss | ~1.4 (validation) |

---


## ğŸ§  Lessons Learned
- Understood **transformer internals** and **scaling laws** through hands-on experimentation.  
- Gained intuition about **context windows**, **attention bottlenecks**, and **training stability**.   



## ğŸ—ï¸ Project Structure
```
LLM
â”œâ”€â”€ Code/
â”‚   â”œâ”€â”€ preprocess.ipynb          # Notebook for preparing the dataset
â”‚   â”œâ”€â”€ train_test.ipynb          # Notebook for training and evaluating the model
â”‚   â””â”€â”€ train_test.py             # Script version of training and evaluation pipeline
â”œâ”€â”€ Dataset/
â”‚   â””â”€â”€ 0000.parquet              # First part of the raw TinyStories dataset used for training
â”‚   â””â”€â”€ text.txt                  # parquet file processed to plain text.
â”œâ”€â”€ SavedModel/
â”‚   â””â”€â”€ model.pth                 # Directory containing trained model weights
â”œâ”€â”€ convergence.png               # Loss curves (training and validation)
â””â”€â”€ README.md
â””â”€â”€ LICENSE
``` 

---

## ğŸ“š References 
- Karpathy, *Let's build GPT: from scratch, in code, spelled out.* 


---

## ğŸ‘¨â€ğŸ’» Author
**Saeed Mohseni**  
Graduate Researcher, Institute for Advanced Computing  
Virginia Tech, VA, USA  

ğŸŒ [Website](https://saeedmohseni.netlify.app/) | ğŸ“« saeedmohseni@vt.edu  

---

## ğŸŒŸ If you like this project...
â­ **Star** the repository  
ğŸ´ **Fork** it  
ğŸ§  **Discuss** ideas or improvements  
